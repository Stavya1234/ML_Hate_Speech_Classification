{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Task 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Notations, Data Initialization** \n",
        "**n** →number of features\n",
        "\n",
        "**m** →number of training examples\n",
        "\n",
        "**X** →input data matrix of shape (m x n)\n",
        "\n",
        "**y** →true/target value (can be 0 or 1 only)\n",
        "\n",
        "**x(i), y(i)** →ith training example\n",
        "\n",
        "**w** → weights (parameters) of shape (n x 1)\n",
        "\n",
        "**b** →bias (parameter), a real number that can be broadcasted.\n",
        "\n",
        "**y_hat**(y with a cap/hat)→ hypothesis (outputs values between 0 and 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Load and preprocess data\n",
        "X_df = pd.read_csv('train_tfidf_features.csv') #train.csv already turned into tfidf features (including id and label)\n",
        "y_df = pd.read_csv('train.csv')['label'] #Extract labels from train.csv\n",
        "\n",
        "# Remove 'id' and 'label' columns from X_df\n",
        "X_df = X_df.drop(['id', 'label'], axis=1)\n",
        "\n",
        "X = X_df.values\n",
        "X_scaled = scaler.fit_transform(X) #Scale X\n",
        "y = y_df.values.reshape(-1, 1) #Reshape y to 2D array\n",
        "y2 = y_df.values #y2 is 1D array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "X_scaled.shape, y2.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Training Functions for Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class LogisticRegression:\n",
        "    def __init__(self, learning_rate=0.01, num_iterations=500, reg_type='l2', lambda_param=0.1, l1_ratio=0.5):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.reg_type = reg_type\n",
        "        self.lambda_param = lambda_param\n",
        "        self.l1_ratio = l1_ratio\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    \n",
        "    def loss(self, y, y_predicted):\n",
        "        return -np.mean(y * np.log(y_predicted + 1e-15) + (1 - y) * np.log(1 - y_predicted + 1e-15))\n",
        "    \n",
        "    def gradients(self, X, y, y_predicted):\n",
        "        num_samples = X.shape[0]\n",
        "        dw = (1 / num_samples) * np.dot(X.T, (y_predicted - y))\n",
        "        db = (1 / num_samples) * np.sum(y_predicted - y)\n",
        "        return dw, db\n",
        "    \n",
        "    def relu(self, z):\n",
        "        return np.maximum(0, z)\n",
        "\n",
        "    def initialize_parameters(self, num_features):\n",
        "        self.weights = np.zeros((num_features,1)) #13747,1\n",
        "        self.bias = 0\n",
        "\n",
        "    def regularization(self):\n",
        "        if self.reg_type == 'l1': #L1 regularization\n",
        "            return self.lambda_param * np.sum(np.abs(self.weights))\n",
        "        elif self.reg_type == 'l2': #L2 regularization\n",
        "            return 0.5 * self.lambda_param * np.sum(self.weights**2)\n",
        "        elif self.reg_type == 'elastic_net': #Elastic Net regularization\n",
        "            l1_term = self.l1_ratio * np.sum(np.abs(self.weights))\n",
        "            l2_term = (1 - self.l1_ratio) * 0.5 * np.sum(self.weights**2)\n",
        "            return self.lambda_param * (l1_term + l2_term)\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def regularization_gradient(self):\n",
        "        if self.reg_type == 'l1':\n",
        "            return self.lambda_param * np.sign(self.weights)\n",
        "        elif self.reg_type == 'l2':\n",
        "            return self.lambda_param * self.weights\n",
        "        elif self.reg_type == 'elastic_net':\n",
        "            l1_grad = self.l1_ratio * np.sign(self.weights)\n",
        "            l2_grad = (1 - self.l1_ratio) * self.weights\n",
        "            return self.lambda_param * (l1_grad + l2_grad)\n",
        "        else:\n",
        "            return np.zeros_like(self.weights)\n",
        "\n",
        "    def train(self, X, y, tolerance=1e-4, max_unchanged_iterations=5):\n",
        "        num_samples, num_features = X.shape\n",
        "        self.initialize_parameters(num_features)\n",
        "\n",
        "        prev_loss = float('inf')\n",
        "        unchanged_iterations = 0\n",
        "        batch_size = 32\n",
        "\n",
        "        for iteration in range(self.num_iterations):\n",
        "            total_loss = 0\n",
        "            # Shuffle the data\n",
        "            indices = np.random.permutation(num_samples)\n",
        "            X_shuffled = X[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            for i in range(0, num_samples, batch_size):\n",
        "                X_batch = X_shuffled[i:i+batch_size]\n",
        "                y_batch = y_shuffled[i:i+batch_size]\n",
        "\n",
        "                # Forward pass\n",
        "                linear_model = np.dot(X_batch, self.weights) + self.bias\n",
        "                y_predicted = self.sigmoid(linear_model)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = self.loss(y_batch, y_predicted)\n",
        "                loss += self.regularization()\n",
        "                total_loss += loss\n",
        "\n",
        "                # Compute gradients\n",
        "                dw, db = self.gradients(X_batch, y_batch, y_predicted)\n",
        "\n",
        "                # Update parameters\n",
        "                self.weights -= self.learning_rate * dw\n",
        "                self.bias -= self.learning_rate * db\n",
        "\n",
        "            # Calculate average loss for this iteration\n",
        "            avg_loss = total_loss / (num_samples / batch_size)\n",
        "\n",
        "            # Check for convergence\n",
        "            if abs(prev_loss - avg_loss) < tolerance:\n",
        "                unchanged_iterations += 1\n",
        "                if unchanged_iterations >= max_unchanged_iterations:\n",
        "                    print(f\"Converged after {iteration + 1} iterations.\")\n",
        "                    break\n",
        "            else:\n",
        "                unchanged_iterations = 0\n",
        "\n",
        "            prev_loss = avg_loss\n",
        "\n",
        "            if (iteration + 1) % 100 == 0:\n",
        "                print(f\"Iteration {iteration + 1}/{self.num_iterations}, Loss: {avg_loss}\")\n",
        "\n",
        "        if iteration == self.num_iterations - 1:\n",
        "            print(f\"Reached maximum iterations ({self.num_iterations}).\")\n",
        "\n",
        "    def predict_prob(self, X):\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        return self.sigmoid(linear_model)\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        return (self.predict_prob(X) >= threshold).astype(int)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
        "\n",
        "\n",
        "# Step 2: Split training data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "\n",
        "# Step 3: Define the LogisticRegression class (as provided in the previous response)\n",
        "# ... (insert the LogisticRegression class definition here)\n",
        "\n",
        "# Step 4: Define a function to train and evaluate the model\n",
        "def train_and_evaluate(X_train, y_train, X_val, y_val, **kwargs):\n",
        "    model = LogisticRegression(**kwargs)\n",
        "    model.train(X_train, y_train)\n",
        "    y_pred = model.predict(X_val)\n",
        "    y_pred_prob = model.predict_prob(X_val)\n",
        "    return accuracy_score(y_val, y_pred), model\n",
        "\n",
        "# Step 5: Hyperparameter tuning\n",
        "num_iterations = 1000\n",
        "regularization_types = ['l2'] #, 'elastic_net', 'l1'\n",
        "lambda_params = [0.0001, 0.001, 0.01, 0.1, 1, 10]  # Regularization parameter\n",
        "learning_rates = [1, 0.1, 0.01, 0.001, 0.0001] # Learning rate\n",
        "l1_ratios = [0.2, 0.5, 0.8]  # Only for elastic net\n",
        "\n",
        "best_accuracy = 0\n",
        "best_params = {}\n",
        "for learning_rate in learning_rates:\n",
        "    for reg_type in regularization_types:\n",
        "        for lambda_param in lambda_params:\n",
        "            # if reg_type == 'elastic_net':\n",
        "            #     for l1_ratio in l1_ratios:\n",
        "            #         accuracy, model = train_and_evaluate(\n",
        "            #             X_train, y_train, X_val, y_val,\n",
        "            #             learning_rate=learning_rate, reg_type=reg_type, lambda_param=lambda_param, l1_ratio=l1_ratio\n",
        "            #         )\n",
        "            #         print(f\"AUC: {accuracy}, Params: {reg_type}, {lambda_param}\")\n",
        "            #         if accuracy > best_accuracy:\n",
        "            #             best_accuracy = accuracy\n",
        "            #             best_params = {'learning_rate':learning_rate, 'reg_type': reg_type, 'lambda_param': lambda_param, 'l1_ratio': l1_ratio}\n",
        "            # else:\n",
        "                accuracy, model = train_and_evaluate(\n",
        "                    X_train, y_train, X_val, y_val,\n",
        "                    learning_rate=learning_rate, reg_type=reg_type, lambda_param=lambda_param\n",
        "                )\n",
        "                print(f\"Accuracy: {accuracy}, Params: lr-{learning_rate}, rt-{reg_type}, lp-{lambda_param}\")\n",
        "                if accuracy > best_accuracy:\n",
        "                    best_accuracy = accuracy\n",
        "                    best_params = {'learning_rate': learning_rate,'reg_type': reg_type, 'lambda_param': lambda_param}\n",
        "                \n",
        "\n",
        "print(f\"Best validation accuracy: {best_accuracy}\")\n",
        "print(f\"Best parameters: {best_params}\")\n",
        "\n",
        "# Step 6: Train the final model with best parameters\n",
        "final_model = LogisticRegression(**best_params)\n",
        "final_model.train(np.vstack((X_train, X_val)), np.concatenate((y_train, y_val)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "final_model = LogisticRegression(**best_params)\n",
        "final_model.train(np.vstack((X_train, X_val)), np.concatenate((y_train, y_val)))\n",
        "# Step 7: Evaluate on the test set\n",
        "X_test_df = pd.read_csv('test_tfidf_features.csv') #train.csv already turned into tfidf features (including id and label)\n",
        "# Remove 'id' columns from X_df\n",
        "X_test_df = X_test_df.drop(['id'], axis=1)\n",
        "X_test = X_test_df.values\n",
        "test_predictions = final_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression as SklearnLogisticRegression\n",
        "\n",
        "# Step 7: Train and predict using sklearn's LogisticRegression\n",
        "if best_params['reg_type'] == 'l1':\n",
        "    sklearn_model = SklearnLogisticRegression(penalty='l1', C=1/best_params['lambda_param'], solver='liblinear')\n",
        "elif best_params['reg_type'] == 'l2':\n",
        "    sklearn_model = SklearnLogisticRegression(penalty='l2', C=1/best_params['lambda_param'])\n",
        "else:  # elastic_net\n",
        "    sklearn_model = SklearnLogisticRegression(penalty='elasticnet', C=1/best_params['lambda_param'], \n",
        "                                              l1_ratio=best_params['l1_ratio'], solver='saga')\n",
        "\n",
        "sklearn_model.fit(X_train, y_train.reshape(-1))\n",
        "sklearn_test_predictions = sklearn_model.predict(X_test)\n",
        "test_predictions = test_predictions.reshape(-1)\n",
        "print(test_predictions.shape, sklearn_test_predictions.shape)\n",
        "\n",
        "# Step 8: Compare predictions\n",
        "\n",
        "agreement = np.mean(test_predictions == sklearn_test_predictions)\n",
        "print(f\"\\nAgreement between custom model and sklearn model on test set: {agreement:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Step 9: Accuracy report for training set\n",
        "train_predictions = final_model.predict(X_train)\n",
        "train_accuracy = accuracy_score(y_train, train_predictions)\n",
        "sklearn_accuracy = accuracy_score(y_train, sklearn_model.predict(X_train))\n",
        "print(f\"\\nTraining Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Training Accuracy (Sklearn): {sklearn_accuracy:.4f}\")\n",
        "print(\"\\nClassification Report (Training Set):\")\n",
        "print(classification_report(y_train, train_predictions))\n",
        "print(\"\\nConfusion Matrix (Training Set):\")\n",
        "print(confusion_matrix(y_train, train_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Step 10: Save test predictions to CSV\n",
        "test_predictions_df = pd.DataFrame(test_predictions, columns=['label'])\n",
        "#sklearn_test_predictions_df = pd.DataFrame(sklearn_test_predictions, columns=['Predicted (Sklearn)'])\n",
        "#Combine the two dataframes\n",
        "#test_predictions_df = pd.concat([test_predictions_df, sklearn_test_predictions_df], axis=1)\n",
        "#Add label column to the dataframe, copying the 'id' column from the test.csv file, placing it as the first column\n",
        "test_predictions_df['id'] = pd.read_csv('test.csv')['id']\n",
        "test_predictions_df = test_predictions_df[['id', 'label']]\n",
        "test_predictions_df.to_csv('t1g3_test_predictions.csv', index=False)\n",
        "print(\"\\nTest predictions saved to 't1g3_test_predictions.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "lambda_params = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]  # Updated lambda parameters\n",
        "for lamb in lambda_params:\n",
        "    alt_model = LogisticRegression(learning_rate=0.1, num_iterations=1000, reg_type='l2', lambda_param=lamb)\n",
        "    alt_model.train(np.vstack((X_train, X_val)), np.concatenate((y_train, y_val)))\n",
        "    alt_test_predictions = alt_model.predict(X_test)\n",
        "    sklearn_model = SklearnLogisticRegression(penalty='l2', C=1/lamb)\n",
        "    sklearn_model.fit(np.vstack((X_train, X_val)), np.concatenate((y_train, y_val)).reshape(-1))\n",
        "    sklearn_test_predictions = sklearn_model.predict(X_test)\n",
        "    agreement = np.mean(alt_test_predictions == sklearn_test_predictions)\n",
        "    print(f\"\\nAgreement between custom model and sklearn model on test set (lambda={lamb}): {agreement:.4f}\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_val, alt_model.predict(X_val)):.4f}\")\n",
        "    print(f\"Accuracy (Sklearn): {accuracy_score(y_val, sklearn_model.predict(X_val)):.4f}\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### with 2000 components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load data\n",
        "df_train = pd.read_csv(\"C:\\\\Users\\\\user\\\\Downloads\\\\train_tfidf_features.csv.zip\")\n",
        "df_test = pd.read_csv(\"C:\\\\Users\\\\user\\\\Downloads\\\\test_tfidf_features.csv (1).zip\")\n",
        "\n",
        "# Split features and labels\n",
        "X_train = df_train.drop(columns=['id', 'label'])\n",
        "y_train = df_train['label']\n",
        "X_test = df_test.drop(columns=['id'])\n",
        "\n",
        "# Scale data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define PCA components to test\n",
        "n_components = 2000  # Change this to the desired number of components\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=n_components, random_state=42)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Train KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=2)\n",
        "knn.fit(X_train_pca, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = knn.predict(X_test_pca)\n",
        "\n",
        "# Create a DataFrame to save predictions\n",
        "submission_df = pd.DataFrame({\n",
        "    'id': df_test['id'],\n",
        "    'label': y_pred\n",
        "})\n",
        "\n",
        "# Save predictions to CSV\n",
        "submission_df.to_csv(\"knn_pca_predictions.csv\", index=False)\n",
        "\n",
        "print(\"Predictions saved to knn_pca_predictions.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### with 1000 components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load data\n",
        "df_train = pd.read_csv(\"C:\\\\Users\\\\user\\\\Downloads\\\\train_tfidf_features.csv.zip\")\n",
        "df_test = pd.read_csv(\"C:\\\\Users\\\\user\\\\Downloads\\\\test_tfidf_features.csv (1).zip\")\n",
        "\n",
        "# Split features and labels\n",
        "X_train = df_train.drop(columns=['id', 'label'])\n",
        "y_train = df_train['label']\n",
        "X_test = df_test.drop(columns=['id'])\n",
        "\n",
        "# Scale data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define PCA components to test\n",
        "n_components = 1000  # Change this to the desired number of components\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=n_components, random_state=42)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Train KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=2)\n",
        "knn.fit(X_train_pca, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = knn.predict(X_test_pca)\n",
        "\n",
        "# Create a DataFrame to save predictions\n",
        "submission_df = pd.DataFrame({\n",
        "    'id': df_test['id'],\n",
        "    'label': y_pred\n",
        "})\n",
        "\n",
        "# Save predictions to CSV\n",
        "submission_df.to_csv(\"knn_pca_predictions.csv\", index=False)\n",
        "\n",
        "print(\"Predictions saved to knn_pca_predictions.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### with 500 components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load data\n",
        "df_train = pd.read_csv(\"C:\\\\Users\\\\user\\\\Downloads\\\\train_tfidf_features.csv.zip\")\n",
        "df_test = pd.read_csv(\"C:\\\\Users\\\\user\\\\Downloads\\\\test_tfidf_features.csv (1).zip\")\n",
        "\n",
        "# Split features and labels\n",
        "X_train = df_train.drop(columns=['id', 'label'])\n",
        "y_train = df_train['label']\n",
        "X_test = df_test.drop(columns=['id'])\n",
        "\n",
        "# Scale data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define PCA components to test\n",
        "n_components = 500  # Change this to the desired number of components\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=n_components, random_state=42)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Train KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=2)\n",
        "knn.fit(X_train_pca, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = knn.predict(X_test_pca)\n",
        "\n",
        "# Create a DataFrame to save predictions\n",
        "submission_df = pd.DataFrame({\n",
        "    'id': df_test['id'],\n",
        "    'label': y_pred\n",
        "})\n",
        "\n",
        "# Save predictions to CSV\n",
        "submission_df.to_csv(\"knn_pca_predictions.csv\", index=False)\n",
        "\n",
        "print(\"Predictions saved to knn_pca_predictions.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### with 100 components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load data\n",
        "df_train = pd.read_csv(\"C:\\\\Users\\\\user\\\\Downloads\\\\train_tfidf_features.csv.zip\")\n",
        "df_test = pd.read_csv(\"C:\\\\Users\\\\user\\\\Downloads\\\\test_tfidf_features.csv (1).zip\")\n",
        "\n",
        "# Split features and labels\n",
        "X_train = df_train.drop(columns=['id', 'label'])\n",
        "y_train = df_train['label']\n",
        "X_test = df_test.drop(columns=['id'])\n",
        "\n",
        "# Scale data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define PCA components to test\n",
        "n_components = 100  # Change this to the desired number of components\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=n_components, random_state=42)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Train KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=2)\n",
        "knn.fit(X_train_pca, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = knn.predict(X_test_pca)\n",
        "\n",
        "# Create a DataFrame to save predictions\n",
        "submission_df = pd.DataFrame({\n",
        "    'id': df_test['id'],\n",
        "    'label': y_pred\n",
        "})\n",
        "\n",
        "# Save predictions to CSV\n",
        "submission_df.to_csv(\"knn_pca_predictions.csv\", index=False)\n",
        "\n",
        "print(\"Predictions saved to knn_pca_predictions.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Task 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "#Task 3 Naive Bayes\n",
        "\n",
        "#load dataset, separate into training and test datasets, features and labels\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "df_train = pd.read_csv(\"C:\\\\Users\\\\user\\\\Downloads\\\\train_tfidf_features.csv.zip\")\n",
        "df_test = pd.read_csv(\"C:\\\\Users\\\\user\\\\Downloads\\\\test_tfidf_features.csv (1).zip\")\n",
        "X_train=df_train.drop(columns=['id','label']) \n",
        "y_train=df_train['label']\n",
        "X_test = df_test.drop(columns=['id'])\n",
        "\n",
        "# Train the Naive Bayes model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "#Prediction\n",
        "y_pred = model.predict(X_test)\n",
        "print(y_pred)\n",
        "submission = pd.DataFrame({'id': df_test['id'], 'label': y_pred})\n",
        "submission.to_csv('task3_version1_naivebayes.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adaboost Classifier with hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Task 3 Adaboost Classifier with hyperparameter tuning\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"C:\\\\Users\\\\user\\\\Downloads\\\\train_tfidf_features.csv.zip\")\n",
        "df_test = pd.read_csv(\"C:\\\\Users\\\\user\\\\Downloads\\\\test_tfidf_features.csv (1).zip\")\n",
        "\n",
        "# Prepare features and labels\n",
        "X_train = df_train.drop(columns=['id', 'label'])\n",
        "y_train = df_train['label']\n",
        "X_test = df_test.drop(columns=['id'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train_set, X_validate_set, y_train_set, y_validate_set = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n",
        "\n",
        "print(f\"Training set shape: {X_train_set.shape}\")\n",
        "print(f\"Validation set shape: {X_validate_set.shape}\")\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200, 500, 1000],\n",
        "    'learning_rate': [0.01, 0.1, 0.5, 1]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=AdaBoostClassifier(random_state=42), param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train_set, y_train_set)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(f\"Best parameters: {best_params}\")\n",
        "print(f\"Best cross-validation accuracy: {best_score}\")\n",
        "\n",
        "# Evaluate the model with the best parameters on the validation set\n",
        "best_model = grid_search.best_estimator_\n",
        "validate_accuracy = best_model.score(X_validate_set, y_validate_set)\n",
        "\n",
        "print(f\"Validation Accuracy with best model: {validate_accuracy}\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Create the submission file\n",
        "submission = pd.DataFrame({'id': df_test['id'], 'label': y_pred})\n",
        "submission.to_csv('task3_adaboostclassifier.csv', index=False)\n",
        "\n",
        "print(\"Submission file created: task3_adaboostclassifier.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ensemble Model with Voting Classifier \n",
        "### (Logistic Regression, XGBClassifier, RandomForestClassifier) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "### Task 3 Ensemble Model with Voting Classifier (Logistic Regression, XGBClassifier, RandomForestClassifier) \n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Loading train and test data sets in dataframe from CSV files\n",
        "df_train = pd.read_csv(\"train_tfidf_features.csv\")\n",
        "df_test = pd.read_csv(\"test_tfidf_features.csv\")\n",
        "\n",
        "X_train = df_train.drop(columns=['id', 'label'])\n",
        "y_train = (df_train['label'] >= 0.5).astype(int)\n",
        "X_test = df_test.drop(columns=['id'])\n",
        "\n",
        "# Splitting the training data into training and validation datasets\n",
        "#X_train_set, X_validate_set, y_train_set, y_validate_set = train_test_split(X_train, y_train, test_size=0.35, random_state=42)\n",
        "\n",
        "# Define parameter grids for each model\n",
        "param_grid_lr = {'C': [0.1, 1, 10]}\n",
        "param_grid_xgb = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1]}\n",
        "param_grid_rf = {'n_estimators': [50, 100, 200]}\n",
        "\n",
        "# Initialize grid search with stratified k-fold cross-validation\n",
        "cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
        "\n",
        "grid_search_lr = GridSearchCV(LogisticRegression(solver='liblinear'), param_grid_lr, cv=cv, scoring='neg_log_loss', verbose=4)\n",
        "grid_search_xgb = GridSearchCV(XGBClassifier(), param_grid_xgb, cv=cv, scoring='roc_auc', verbose=4)\n",
        "grid_search_rf = GridSearchCV(RandomForestClassifier(), param_grid_rf, cv=cv, scoring='accuracy', verbose=4)\n",
        "\n",
        "# Fit grid search\n",
        "grid_search_lr.fit(X_train, y_train)\n",
        "grid_search_xgb.fit(X_train, y_train)\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "# Get the best models\n",
        "best_lr = grid_search_lr.best_estimator_\n",
        "best_xgb = grid_search_xgb.best_estimator_\n",
        "best_rf = grid_search_rf.best_estimator_\n",
        "\n",
        "# Print the best parameters for each model\n",
        "print(\"Best parameters for Logistic Regression:\", grid_search_lr.best_params_)\n",
        "print(\"Best parameters for XGBoost:\", grid_search_xgb.best_params_)\n",
        "print(\"Best parameters for Random Forest:\", grid_search_rf.best_params_)\n",
        "\n",
        "# Create a voting classifier with the best models\n",
        "final_model = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('lr', best_lr),\n",
        "        ('xgb', best_xgb),\n",
        "        ('rf', best_rf)\n",
        "    ],\n",
        "    voting='soft',\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Train the final voting classifier on the training set\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "# Print the accuracy of the model on the training set\n",
        "train_accuracy = final_model.score(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_test_pred = final_model.predict(X_test)\n",
        "\n",
        "# Generate the submission file\n",
        "submission = pd.DataFrame({'id': df_test['id'], 'label': y_test_pred})\n",
        "submission.to_csv('voting_classifier.csv', index=False)\n",
        "\n",
        "print(\"Submission files created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ensemble Model with Logistic Regression, XGBClassifier, RandomForestClassifier \n",
        "### (Static Classifier was used as the final classifier) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-8b9409086bd9>:1: DeprecationWarning: \n",
            "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
            "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
            "but was not found to be installed on your system.\n",
            "If this would cause problems for you,\n",
            "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
            "        \n",
            "  import pandas as pd\n"
          ]
        },
        {
          "ename": "<class 'ModuleNotFoundError'>",
          "evalue": "No module named 'skopt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskopt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BayesSearchCV\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     12\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skopt'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import make_scorer, accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier, GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from skopt import BayesSearchCV\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv(\"C:\\\\Users\\\\user\\\\Downloads\\\\train_tfidf_features.csv.zip\")\n",
        "df_test = pd.read_csv(\"C:\\\\Users\\\\user\\\\Downloads\\\\test_tfidf_features.csv (1).zip\")\n",
        "\n",
        "# Prepare features and labels\n",
        "X_train = df_train.drop(columns=['id', 'label'])\n",
        "y_train = df_train['label']\n",
        "X_test = df_test.drop(columns=['id'])\n",
        "\n",
        "# Split the data into training and validation sets for hyperparameter tuning\n",
        "X_train_set, X_validate_set, y_train_set, y_validate_set = train_test_split(\n",
        "    X_train, y_train, test_size=0.3, random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "print(f\"Training set shape: {X_train_set.shape}\")\n",
        "print(f\"Validation set shape: {X_validate_set.shape}\")\n",
        "\n",
        "# Define parameter grids for each model\n",
        "param_dist_lr = {'lr__C': [0.01, 0.1, 1], 'lr__solver': ['lbfgs'], 'lr__max_iter': [100, 200]}\n",
        "param_dist_xgb = {\n",
        "    'n_estimators': [50, 100], \n",
        "    'learning_rate': [0.01, 0.1], \n",
        "    'max_depth': [3, 6], \n",
        "    'reg_lambda': [1, 10],\n",
        "    'reg_alpha': [0, 1]\n",
        "}\n",
        "param_dist_rf = {'rf__n_estimators': [50, 100], 'rf__max_depth': [10, 20], 'rf__min_samples_split': [2, 5]}\n",
        "param_dist_gb = {\n",
        "    'gb__n_estimators': (50, 100), \n",
        "    'gb__learning_rate': (0.01, 0.1), \n",
        "    'gb__max_depth': (3, 6)\n",
        "}\n",
        "\n",
        "# Initialize the models\n",
        "model_lr = LogisticRegression()\n",
        "model_xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "model_rf = RandomForestClassifier()\n",
        "model_gb = GradientBoostingClassifier()\n",
        "\n",
        "# Create pipelines for model training\n",
        "pipeline_lr = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('lr', model_lr)\n",
        "])\n",
        "\n",
        "pipeline_rf = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('rf', model_rf)\n",
        "])\n",
        "\n",
        "pipeline_gb = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('gb', model_gb)\n",
        "])\n",
        "\n",
        "# Initialize BayesianSearchCV for the Gradient Boosting model\n",
        "bayes_search_gb = BayesSearchCV(\n",
        "    estimator=pipeline_gb, \n",
        "    search_spaces=param_dist_gb, \n",
        "    n_iter=8, \n",
        "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), \n",
        "    scoring='accuracy', \n",
        "    n_jobs=1, \n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Initialize RandomizedSearchCV for other models\n",
        "random_search_lr = RandomizedSearchCV(\n",
        "    estimator=pipeline_lr, param_distributions=param_dist_lr, n_iter=6, \n",
        "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), \n",
        "    scoring='accuracy', n_jobs=1, verbose=1\n",
        ")\n",
        "random_search_xgb = RandomizedSearchCV(\n",
        "    estimator=model_xgb, param_distributions=param_dist_xgb, n_iter=10, \n",
        "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), \n",
        "    scoring='accuracy', n_jobs=1, verbose=1\n",
        ")\n",
        "random_search_rf = RandomizedSearchCV(\n",
        "    estimator=pipeline_rf, param_distributions=param_dist_rf, n_iter=8, \n",
        "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), \n",
        "    scoring='accuracy', n_jobs=1, verbose=1\n",
        ")\n",
        "\n",
        "# Custom scoring function for XGBoost with early stopping\n",
        "def xgb_scorer(estimator, X, y):\n",
        "    estimator.fit(X, y, eval_set=[(X_validate_set, y_validate_set)], early_stopping_rounds=10, verbose=False)\n",
        "    return accuracy_score(y, estimator.predict(X))\n",
        "\n",
        "# Fit RandomizedSearchCV\n",
        "print(\"Training Logistic Regression model...\")\n",
        "random_search_lr.fit(X_train_set, y_train_set)\n",
        "\n",
        "print(\"Training XGBoost model...\")\n",
        "random_search_xgb.scoring = make_scorer(xgb_scorer, greater_is_better=True)\n",
        "random_search_xgb.fit(X_train_set, y_train_set)\n",
        "\n",
        "print(\"Training Random Forest model...\")\n",
        "random_search_rf.fit(X_train_set, y_train_set)\n",
        "\n",
        "print(\"Training Gradient Boosting model with Bayesian optimization...\")\n",
        "bayes_search_gb.fit(X_train_set, y_train_set)\n",
        "\n",
        "# Get the best models\n",
        "best_lr = random_search_lr.best_estimator_\n",
        "best_xgb = random_search_xgb.best_estimator_\n",
        "best_rf = random_search_rf.best_estimator_\n",
        "best_gb = bayes_search_gb.best_estimator_\n",
        "\n",
        "print(f\"Best parameters for Logistic Regression: {random_search_lr.best_params_}\")\n",
        "print(f\"Best parameters for XGBoost: {random_search_xgb.best_params_}\")\n",
        "print(f\"Best parameters for Random Forest: {random_search_rf.best_params_}\")\n",
        "print(f\"Best parameters for Gradient Boosting: {bayes_search_gb.best_params_}\")\n",
        "\n",
        "# Create the Voting Classifier with the best models\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', best_lr), ('xgb', best_xgb), ('rf', best_rf), ('gb', best_gb)],\n",
        "    voting='soft'  # Using soft voting to leverage probability estimates\n",
        ")\n",
        "\n",
        "# Stack the models using a meta-classifier\n",
        "stacked_clf = StackingClassifier(\n",
        "    estimators=[('lr', best_lr), ('xgb', best_xgb), ('rf', best_rf), ('gb', best_gb)],\n",
        "    final_estimator=LogisticRegression(C=0.1, max_iter=100)  # Increased regularization for meta-classifier\n",
        ")\n",
        "\n",
        "# Fit the Stacking Classifier on the entire training data\n",
        "print(\"Training Stacking Classifier...\")\n",
        "stacked_clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the Stacking Classifier\n",
        "train_accuracy = stacked_clf.score(X_train_set, y_train_set)\n",
        "val_accuracy = stacked_clf.score(X_validate_set, y_validate_set)\n",
        "\n",
        "print(f\"Stacking Classifier Training accuracy: {train_accuracy}\")\n",
        "print(f\"Stacking Classifier Validation accuracy: {val_accuracy}\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_test_pred = stacked_clf.predict(X_test)\n",
        "\n",
        "# Create the submission file\n",
        "submission = pd.DataFrame({'id': df_test['id'], 'label': y_test_pred})\n",
        "submission.to_csv('stacking_classifier_submission_v3.csv', index=False)\n",
        "\n",
        "print(\"Submission file created: stacking_classifier_submission_v3.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
